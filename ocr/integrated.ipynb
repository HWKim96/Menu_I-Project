{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#닭칼국수 이미지 Craft로 좌표 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.py 실행 코드\n",
    "#python test.py --trained_model=[weightfile] --test_folder=[folder path to test images]\n",
    "#python test.py --trained_model=craft_mlt_25k.pth --test_folder=./test\n",
    "#이미지를 전처리 해서 넣는다면? 기울기를 조절할 수 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#고정된다면?\n",
    "trained_model = './craft_mlt_25k.pth'\n",
    "test_folder = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import craft_utils\n",
    "import imgproc\n",
    "import file_utils\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from craft import CRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "#copyStateDict 함수는 PyTorch 모델의 state_dict에서 불필요한 prefix (예: \"module.\")를 제거하는 역할\n",
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "#str2bool 함수는 문자열을 boolean 값으로 변환해주는 함수입니다. 대소문자를 구분하지 않으며, \"yes\", \"y\", \"true\", \"t\", \"1\" 중 하나가 입력되면 True를 반환합니다. 그 외에는 False를 반환합니다. 이 함수는 일반적으로 argparse 모듈과 함께 사용되어, 스크립트 실행 시 입력되는 인자값을 boolean 형태로 변환해주는데 사용\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--text_threshold TEXT_THRESHOLD]\n",
      "                             [--low_text LOW_TEXT]\n",
      "                             [--link_threshold LINK_THRESHOLD] [--cuda CUDA]\n",
      "                             [--canvas_size CANVAS_SIZE]\n",
      "                             [--mag_ratio MAG_RATIO] [--poly] [--show_time]\n",
      "                             [--refine] [--refiner_model REFINER_MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3926664f-9eb7-4f26-97f3-c4f433f08868\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\SBAUser\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-553221TJUwjGRUAR.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#import argparse\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='CRAFT Text Detection')\n",
    "#parser.add_argument('--trained_model', default='weights/craft_mlt_25k.pth', type=str, help='pretrained model')\n",
    "# parser.add_argument('--text_threshold', default=0.7, type=float, help='text confidence threshold')\n",
    "# parser.add_argument('--low_text', default=0.4, type=float, help='text low-bound score')\n",
    "# parser.add_argument('--link_threshold', default=0.4, type=float, help='link confidence threshold')\n",
    "# parser.add_argument('--cuda', default=True, type=str2bool, help='Use cuda for inference')\n",
    "# parser.add_argument('--canvas_size', default=1280, type=int, help='image size for inference')\n",
    "# parser.add_argument('--mag_ratio', default=1.5, type=float, help='image magnification ratio')\n",
    "# parser.add_argument('--poly', default=False, action='store_true', help='enable polygon type')\n",
    "# parser.add_argument('--show_time', default=False, action='store_true', help='show processing time')\n",
    "#parser.add_argument('--test_folder', default='/data/', type=str, help='folder path to input images')\n",
    "# parser.add_argument('--refine', default=False, action='store_true', help='enable link refiner')\n",
    "# parser.add_argument('--refiner_model', default='weights/craft_refiner_CTW1500.pth', type=str, help='pretrained refiner model')\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    #\"trained_model\": \"weights/craft_mlt_25k.pth\",\n",
    "    \"text_threshold\": 0.7,\n",
    "    \"low_text\": 0.4,\n",
    "    \"link_threshold\": 0.4,\n",
    "    \"cuda\": True,\n",
    "    \"canvas_size\": 1280,\n",
    "    \"mag_ratio\": 1.5,\n",
    "    \"poly\": False,\n",
    "    \"show_time\": False,\n",
    "    #\"test_folder\": \"/data/\",\n",
    "    \"refine\": False,\n",
    "    \"refiner_model\": \"weights/craft_refiner_CTW1500.pth\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" For test images in a folder \"\"\"\n",
    "image_list, _, _ = file_utils.get_files(test_folder)\n",
    "\n",
    "result_folder = './result/'\n",
    "if not os.path.isdir(result_folder):\n",
    "    os.mkdir(result_folder)\n",
    "\n",
    "def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # resize\n",
    "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "    # preprocessing\n",
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "    if cuda:\n",
    "        x = x.cuda()\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        y, feature = net(x)\n",
    "\n",
    "    # make score and link map\n",
    "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "\n",
    "    # refine link\n",
    "    if refine_net is not None:\n",
    "        with torch.no_grad():\n",
    "            y_refiner = refine_net(y, feature)\n",
    "        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
    "\n",
    "    t0 = time.time() - t0\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Post-processing\n",
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n",
    "\n",
    "    # coordinate adjustment\n",
    "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "    for k in range(len(polys)):\n",
    "        if polys[k] is None: polys[k] = boxes[k]\n",
    "\n",
    "    t1 = time.time() - t1\n",
    "\n",
    "    # render results (optional)\n",
    "    render_img = score_text.copy()\n",
    "    render_img = np.hstack((render_img, score_link))\n",
    "    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
    "\n",
    "    if args.show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
    "\n",
    "    return boxes, polys, ret_score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:252: UserWarning: Accessing the model URLs via the internal dictionary of the module is deprecated since 0.13 and may be removed in the future. Please access them via the appropriate Weights Enum instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SBAUser\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SBAUser\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from checkpoint (./craft_mlt_25k.pth)\n",
      "[[[ 42.291588  40.052258]st_dak.jpg\n",
      "  [270.59546   20.734224]\n",
      "  [274.67725   68.97346 ]\n",
      "  [ 46.37337   88.2915  ]]\n",
      "\n",
      " [[ 33.77578  113.303856]\n",
      "  [272.1829    97.410034]\n",
      "  [275.6401   149.26843 ]\n",
      "  [ 37.233013 165.16225 ]]\n",
      "\n",
      " [[ 27.119066 196.93988 ]\n",
      "  [272.2465   181.45815 ]\n",
      "  [275.681    235.8377  ]\n",
      "  [ 30.553549 251.31944 ]]\n",
      "\n",
      " [[ 11.187969 285.1397  ]\n",
      "  [275.17435  272.32486 ]\n",
      "  [278.02872  331.12482 ]\n",
      "  [ 14.042338 343.9397  ]]] [array([[ 42.291588,  40.052258],\n",
      "        [270.59546 ,  20.734224],\n",
      "        [274.67725 ,  68.97346 ],\n",
      "        [ 46.37337 ,  88.2915  ]], dtype=float32)\n",
      " array([[ 33.77578 , 113.303856],\n",
      "        [272.1829  ,  97.410034],\n",
      "        [275.6401  , 149.26843 ],\n",
      "        [ 37.233013, 165.16225 ]], dtype=float32)\n",
      " array([[ 27.119066, 196.93988 ],\n",
      "        [272.2465  , 181.45815 ],\n",
      "        [275.681   , 235.8377  ],\n",
      "        [ 30.553549, 251.31944 ]], dtype=float32)\n",
      " array([[ 11.187969, 285.1397  ],\n",
      "        [275.17435 , 272.32486 ],\n",
      "        [278.02872 , 331.12482 ],\n",
      "        [ 14.042338, 343.9397  ]], dtype=float32)] [[[132   0   0]\n",
      "  [132   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[132   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[132   0   0]\n",
      "  [132   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[132   0   0]\n",
      "  [136   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]\n",
      "\n",
      " [[128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  ...\n",
      "  [128   0   0]\n",
      "  [128   0   0]\n",
      "  [128   0   0]]]\n",
      "elapsed time : 0.17185711860656738s\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__': 이거 치면 이하의 내용 tap 해야함.\n",
    "# load net\n",
    "net = CRAFT()     # initialize\n",
    "\n",
    "print('Loading weights from checkpoint (' + trained_model + ')')\n",
    "if args.cuda:\n",
    "    net.load_state_dict(copyStateDict(torch.load(trained_model)))\n",
    "else:\n",
    "    net.load_state_dict(copyStateDict(torch.load(trained_model, map_location='cpu')))\n",
    "\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# LinkRefiner\n",
    "refine_net = None\n",
    "if args.refine:\n",
    "    from refinenet import RefineNet\n",
    "    refine_net = RefineNet()\n",
    "    print('Loading weights of refiner from checkpoint (' + args.refiner_model + ')')\n",
    "    if args.cuda:\n",
    "        refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model)))\n",
    "        refine_net = refine_net.cuda()\n",
    "        refine_net = torch.nn.DataParallel(refine_net)\n",
    "    else:\n",
    "        refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location='cpu')))\n",
    "\n",
    "    refine_net.eval()\n",
    "    args.poly = True\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# load data\n",
    "for k, image_path in enumerate(image_list):\n",
    "    print(\"Test image {:d}/{:d}: {:s}\".format(k+1, len(image_list), image_path), end='\\r')\n",
    "    image = imgproc.loadImage(image_path)\n",
    "\n",
    "    bboxes, polys, score_text = test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, refine_net)\n",
    "\n",
    "    print(bboxes, polys, score_text)\n",
    "    # save score text\n",
    "    filename, file_ext = os.path.splitext(os.path.basename(image_path))\n",
    "    mask_file = result_folder + \"/res_\" + filename + '_mask.jpg'\n",
    "    cv2.imwrite(mask_file, score_text)\n",
    "\n",
    "    file_utils.saveResult(image_path, image[:,:,::-1], polys, dirname=result_folder)\n",
    "\n",
    "print(\"elapsed time : {}s\".format(time.time() - t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 2)\n",
      "[[ 42.291588  40.052258]\n",
      " [270.59546   20.734224]\n",
      " [274.67725   68.97346 ]\n",
      " [ 46.37337   88.2915  ]]\n",
      "[[ 33.77578  113.303856]\n",
      " [272.1829    97.410034]\n",
      " [275.6401   149.26843 ]\n",
      " [ 37.233013 165.16225 ]]\n",
      "[[ 27.119066 196.93988 ]\n",
      " [272.2465   181.45815 ]\n",
      " [275.681    235.8377  ]\n",
      " [ 30.553549 251.31944 ]]\n",
      "[[ 11.187969 285.1397  ]\n",
      " [275.17435  272.32486 ]\n",
      " [278.02872  331.12482 ]\n",
      " [ 14.042338 343.9397  ]]\n"
     ]
    }
   ],
   "source": [
    "print(bboxes.shape)\n",
    "print(bboxes[0])\n",
    "print(bboxes[1])\n",
    "print(bboxes[2])\n",
    "print(bboxes[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 42.291588,  40.052258],\n",
      "       [270.59546 ,  20.734224],\n",
      "       [274.67725 ,  68.97346 ],\n",
      "       [ 46.37337 ,  88.2915  ]], dtype=float32), array([[ 33.77578 , 113.303856],\n",
      "       [272.1829  ,  97.410034],\n",
      "       [275.6401  , 149.26843 ],\n",
      "       [ 37.233013, 165.16225 ]], dtype=float32), array([[ 27.119066, 196.93988 ],\n",
      "       [272.2465  , 181.45815 ],\n",
      "       [275.681   , 235.8377  ],\n",
      "       [ 30.553549, 251.31944 ]], dtype=float32), array([[ 11.187969, 285.1397  ],\n",
      "       [275.17435 , 272.32486 ],\n",
      "       [278.02872 , 331.12482 ],\n",
      "       [ 14.042338, 343.9397  ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 각 array를 list에 담은 것\n",
    "bboxes_list=[]\n",
    "for i in range(bboxes.shape[0]):\n",
    "  bboxes_list.append(bboxes[i])\n",
    "print(bboxes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 42.291588,  40.052258],\n",
       "        [270.59546 ,  20.734224],\n",
       "        [274.67725 ,  68.97346 ],\n",
       "        [ 46.37337 ,  88.2915  ]],\n",
       "\n",
       "       [[ 33.77578 , 113.303856],\n",
       "        [272.1829  ,  97.410034],\n",
       "        [275.6401  , 149.26843 ],\n",
       "        [ 37.233013, 165.16225 ]],\n",
       "\n",
       "       [[ 27.119066, 196.93988 ],\n",
       "        [272.2465  , 181.45815 ],\n",
       "        [275.681   , 235.8377  ],\n",
       "        [ 30.553549, 251.31944 ]],\n",
       "\n",
       "       [[ 11.187969, 285.1397  ],\n",
       "        [275.17435 , 272.32486 ],\n",
       "        [278.02872 , 331.12482 ],\n",
       "        [ 14.042338, 343.9397  ]]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 42.291588,  40.052258],\n",
       "        [270.59546 ,  20.734224],\n",
       "        [274.67725 ,  68.97346 ],\n",
       "        [ 46.37337 ,  88.2915  ]], dtype=float32),\n",
       " array([[ 33.77578 , 113.303856],\n",
       "        [272.1829  ,  97.410034],\n",
       "        [275.6401  , 149.26843 ],\n",
       "        [ 37.233013, 165.16225 ]], dtype=float32),\n",
       " array([[ 27.119066, 196.93988 ],\n",
       "        [272.2465  , 181.45815 ],\n",
       "        [275.681   , 235.8377  ],\n",
       "        [ 30.553549, 251.31944 ]], dtype=float32),\n",
       " array([[ 11.187969, 285.1397  ],\n",
       "        [275.17435 , 272.32486 ],\n",
       "        [278.02872 , 331.12482 ],\n",
       "        [ 14.042338, 343.9397  ]], dtype=float32)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_0 = bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17060\\851392793.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/test_dak.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'open'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지 로드\n",
    "img = Image.open(test_folder+'/test_dak.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_folder = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1274: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17060\\2451815697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# crop된 이미지 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'crop_img'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.4) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1274: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# 좌표값을 정수형으로 변환\n",
    "#bboxes_0 = bboxes_0.astype(np.int32)\n",
    "\n",
    "# crop할 영역 계산\n",
    "#x1, y1 = np.min(bboxes_0, axis=0)\n",
    "#x2, y2 = np.max(bboxes_0, axis=0)\n",
    "#w, h = x2 - x1, y2 - y1\n",
    "\n",
    "# 이미지에서 crop 영역 추출\n",
    "#crop_img = img[y1:y1+h, x1:x1+w]\n",
    "\n",
    "# crop된 이미지 출력\n",
    "#cv2.imshow('crop_img', crop_img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop할 영역 계산\n",
    "left = min([bbox[0] for bbox in bboxes_0])\n",
    "top = min([bbox[1] for bbox in bboxes_0])\n",
    "right = max([bbox[0] for bbox in bboxes_0])\n",
    "bottom = max([bbox[1] for bbox in bboxes_0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop된 이미지 추출\n",
    "img_cropped = img.crop((left, top, right, bottom))\n",
    "\n",
    "# crop된 이미지 출력\n",
    "img_cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop된 이미지 파일로 저장\n",
    "img_cropped.save('cropped.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img=Image.open('./test/5484_272549341.jpg')\n",
    "\n",
    "#img_resized=img.resize((400,300))\n",
    "\n",
    "#img_resized.show()\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x와 y 좌표를 각각 나눔\n",
    "x=bbox[:,0]\n",
    "y=bbox[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직사각형의 좌표 계산\n",
    "left = np.min(x)\n",
    "top = np.min(y)\n",
    "right = np.max(x)\n",
    "bottom = np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 열어서 크롭\n",
    "img=Image.open('./test/5484_272549341.jpg')\n",
    "img_cropped = img.crop((left, top, right, bottom))\n",
    "img_cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[604.8,244.8],\n",
    "[644.8,244.8],\n",
    "[644.8,273.6],\n",
    "[604.8,273.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=a[:,0]\n",
    "y=a[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직사각형의 좌표 계산\n",
    "left = np.min(x)\n",
    "top = np.min(y)\n",
    "right = np.max(x)\n",
    "bottom = np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 열어서 크롭\n",
    "img=Image.open('./test/5484_272549341.jpg')\n",
    "img_cropped = img.crop((left, top, right, bottom))\n",
    "img_cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= np.array( [[306.91354,263.339],\n",
    "  [534.40765,241.22156],\n",
    "  [539.2254,290.77536],\n",
    "  [311.7313,312.8928,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=c[:,0]\n",
    "y=c[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직사각형의 좌표 계산\n",
    "left = np.min(x)\n",
    "top = np.min(y)\n",
    "right = np.max(x)\n",
    "bottom = np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 열어서 크롭\n",
    "img=Image.open('./test/5484_272549341.jpg')\n",
    "img_cropped = img.crop((left, top, right, bottom))\n",
    "img_cropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('E:\\DL_proj_2jo\\code\\CRAFT-pytorch-master\\CRAFT-pytorch-master')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CTCLabelConverter, AttnLabelConverter\n",
    "from dataset import RawDataset, AlignCollate\n",
    "from model import Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CTCLabelConverter, AttnLabelConverter\n",
    "from dataset import RawDataset, AlignCollate\n",
    "from model import Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "def demo(opt):\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # load model\n",
    "    print('loading pretrained model from %s' % opt.saved_model)\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "\n",
    "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text_for_pred)\n",
    "\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                # preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode(preds_index, preds_size)\n",
    "\n",
    "            else:\n",
    "                preds = model(image, text_for_pred, is_train=False)\n",
    "\n",
    "                # select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "\n",
    "            log = open(f'./log_demo_result.txt', 'a')\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
    "            \n",
    "            print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "            log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "                # calculate confidence score (= multiply of pred_max_prob)\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "\n",
    "                print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
    "                log.write(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n')\n",
    "\n",
    "            log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--image_folder', required=True, help='path to image_folder which contains text images')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
    "parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
    "\"\"\" Data processing \"\"\"\n",
    "parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
    "parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
    "parser.add_argument('--character', type=str, default=' !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmonpqrstuvwxyz{|}~가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰갱갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겄겅겆겉겊겋게겐겔겜겝겟겠겡겨격겪견겯결겸겹겻겼경곁계곈곌곕곗고곡곤곧골곪곬곯곰곱곳공곶과곽관괄괆괌괍괏광괘괜괠괩괬괭괴괵괸괼굄굅굇굉교굔굘굡굣구국군굳굴굵굶굻굼굽굿궁궂궈궉권궐궜궝궤궷귀귁귄귈귐귑귓규균귤그극근귿글긁금급긋긍긔기긱긴긷길긺김깁깃깅깆깊까깍깎깐깔깖깜깝깟깠깡깥깨깩깬깰깸깹깻깼깽꺄꺅꺌꺼꺽꺾껀껄껌껍껏껐껑께껙껜껨껫껭껴껸껼꼇꼈꼍꼐꼬꼭꼰꼲꼴꼼꼽꼿꽁꽂꽃꽈꽉꽐꽜꽝꽤꽥꽹꾀꾄꾈꾐꾑꾕꾜꾸꾹꾼꿀꿇꿈꿉꿋꿍꿎꿔꿜꿨꿩꿰꿱꿴꿸뀀뀁뀄뀌뀐뀔뀜뀝뀨끄끅끈끊끌끎끓끔끕끗끙끝끼끽낀낄낌낍낏낑나낙낚난낟날낡낢남납낫났낭낮낯낱낳내낵낸낼냄냅냇냈냉냐냑냔냘냠냥너넉넋넌널넒넓넘넙넛넜넝넣네넥넨넬넴넵넷넸넹녀녁년녈념녑녔녕녘녜녠노녹논놀놂놈놉놋농높놓놔놘놜놨뇌뇐뇔뇜뇝뇟뇨뇩뇬뇰뇹뇻뇽누눅눈눋눌눔눕눗눙눠눴눼뉘뉜뉠뉨뉩뉴뉵뉼늄늅늉느늑는늘늙늚늠늡늣능늦늪늬늰늴니닉닌닐닒님닙닛닝닢다닥닦단닫달닭닮닯닳담답닷닸당닺닻닿대댁댄댈댐댑댓댔댕댜더덕덖던덛덜덞덟덤덥덧덩덫덮데덱덴델뎀뎁뎃뎄뎅뎌뎐뎔뎠뎡뎨뎬도독돈돋돌돎돐돔돕돗동돛돝돠돤돨돼됐되된될됨됩됫됴두둑둔둘둠둡둣둥둬뒀뒈뒝뒤뒨뒬뒵뒷뒹듀듄듈듐듕드득든듣들듦듬듭듯등듸디딕딘딛딜딤딥딧딨딩딪따딱딴딸땀땁땃땄땅땋때땍땐땔땜땝땟땠땡떠떡떤떨떪떫떰떱떳떴떵떻떼떽뗀뗄뗌뗍뗏뗐뗑뗘뗬또똑똔똘똥똬똴뙈뙤뙨뚜뚝뚠뚤뚫뚬뚱뛔뛰뛴뛸뜀뜁뜅뜨뜩뜬뜯뜰뜸뜹뜻띄띈띌띔띕띠띤띨띰띱띳띵라락란랄람랍랏랐랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝려력련렬렴렵렷렸령례롄롑롓로록론롤롬롭롯롱롸롼뢍뢨뢰뢴뢸룀룁룃룅료룐룔룝룟룡루룩룬룰룸룹룻룽뤄뤘뤠뤼뤽륀륄륌륏륑류륙륜률륨륩륫륭르륵른를름릅릇릉릊릍릎리릭린릴림립릿링마막만많맏말맑맒맘맙맛망맞맡맣매맥맨맬맴맵맷맸맹맺먀먁먈먕머먹먼멀멂멈멉멋멍멎멓메멕멘멜멤멥멧멨멩며멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏뭐뭔뭘뭡뭣뭬뮈뮌뮐뮤뮨뮬뮴뮷므믄믈믐믓미믹민믿밀밂밈밉밋밌밍및밑바박밖밗반받발밝밞밟밤밥밧방밭배백밴밸뱀뱁뱃뱄뱅뱉뱌뱍뱐뱝버벅번벋벌벎범법벗벙벚베벡벤벧벨벰벱벳벴벵벼벽변별볍볏볐병볕볘볜보복볶본볼봄봅봇봉봐봔봤봬뵀뵈뵉뵌뵐뵘뵙뵤뵨부북분붇불붉붊붐붑붓붕붙붚붜붤붰붸뷔뷕뷘뷜뷩뷰뷴뷸븀븃븅브븍븐블븜븝븟비빅빈빌빎빔빕빗빙빚빛빠빡빤빨빪빰빱빳빴빵빻빼빽뺀뺄뺌뺍뺏뺐뺑뺘뺙뺨뻐뻑뻔뻗뻘뻠뻣뻤뻥뻬뼁뼈뼉뼘뼙뼛뼜뼝뽀뽁뽄뽈뽐뽑뽕뾔뾰뿅뿌뿍뿐뿔뿜뿟뿡쀼쁑쁘쁜쁠쁨쁩삐삑삔삘삠삡삣삥사삭삯산삳살삵삶삼삽삿샀상샅새색샌샐샘샙샛샜생샤샥샨샬샴샵샷샹섀섄섈섐섕서석섞섟선섣설섦섧섬섭섯섰성섶세섹센셀셈셉셋셌셍셔셕션셜셤셥셧셨셩셰셴셸솅소속솎손솔솖솜솝솟송솥솨솩솬솰솽쇄쇈쇌쇔쇗쇘쇠쇤쇨쇰쇱쇳쇼쇽숀숄숌숍숏숑수숙순숟술숨숩숫숭숯숱숲숴쉈쉐쉑쉔쉘쉠쉥쉬쉭쉰쉴쉼쉽쉿슁슈슉슐슘슛슝스슥슨슬슭슴습슷승시식신싣실싫심십싯싱싶싸싹싻싼쌀쌈쌉쌌쌍쌓쌔쌕쌘쌜쌤쌥쌨쌩썅써썩썬썰썲썸썹썼썽쎄쎈쎌쏀쏘쏙쏜쏟쏠쏢쏨쏩쏭쏴쏵쏸쐈쐐쐤쐬쐰쐴쐼쐽쑈쑤쑥쑨쑬쑴쑵쑹쒀쒔쒜쒸쒼쓩쓰쓱쓴쓸쓺쓿씀씁씌씐씔씜씨씩씬씰씸씹씻씽아악안앉않알앍앎앓암압앗았앙앝앞애액앤앨앰앱앳앴앵야약얀얄얇얌얍얏양얕얗얘얜얠얩어억언얹얻얼얽얾엄업없엇었엉엊엌엎에엑엔엘엠엡엣엥여역엮연열엶엷염엽엾엿였영옅옆옇예옌옐옘옙옛옜오옥온올옭옮옰옳옴옵옷옹옻와왁완왈왐왑왓왔왕왜왝왠왬왯왱외왹왼욀욈욉욋욍요욕욘욜욤욥욧용우욱운울욹욺움웁웃웅워웍원월웜웝웠웡웨웩웬웰웸웹웽위윅윈윌윔윕윗윙유육윤율윰윱윳융윷으윽은을읊음읍읏응읒읓읔읕읖읗의읜읠읨읫이익인일읽읾잃임입잇있잉잊잎자작잔잖잗잘잚잠잡잣잤장잦재잭잰잴잼잽잿쟀쟁쟈쟉쟌쟎쟐쟘쟝쟤쟨쟬저적전절젊점접젓정젖제젝젠젤젬젭젯젱져젼졀졈졉졌졍졔조족존졸졺좀좁좃종좆좇좋좌좍좔좝좟좡좨좼좽죄죈죌죔죕죗죙죠죡죤죵주죽준줄줅줆줌줍줏중줘줬줴쥐쥑쥔쥘쥠쥡쥣쥬쥰쥴쥼즈즉즌즐즘즙즛증지직진짇질짊짐집짓징짖짙짚짜짝짠짢짤짧짬짭짯짰짱째짹짼쨀쨈쨉쨋쨌쨍쨔쨘쨩쩌쩍쩐쩔쩜쩝쩟쩠쩡쩨쩽쪄쪘쪼쪽쫀쫄쫌쫍쫏쫑쫓쫘쫙쫠쫬쫴쬈쬐쬔쬘쬠쬡쭁쭈쭉쭌쭐쭘쭙쭝쭤쭸쭹쮜쮸쯔쯤쯧쯩찌찍찐찔찜찝찡찢찧차착찬찮찰참찹찻찼창찾채책챈챌챔챕챗챘챙챠챤챦챨챰챵처척천철첨첩첫첬청체첵첸첼쳄쳅쳇쳉쳐쳔쳤쳬쳰촁초촉촌촐촘촙촛총촤촨촬촹최쵠쵤쵬쵭쵯쵱쵸춈추축춘출춤춥춧충춰췄췌췐취췬췰췸췹췻췽츄츈츌츔츙츠측츤츨츰츱츳층치칙친칟칠칡침칩칫칭카칵칸칼캄캅캇캉캐캑캔캘캠캡캣캤캥캬캭컁커컥컨컫컬컴컵컷컸컹케켁켄켈켐켑켓켕켜켠켤켬켭켯켰켱켸코콕콘콜콤콥콧콩콰콱콴콸쾀쾅쾌쾡쾨쾰쿄쿠쿡쿤쿨쿰쿱쿳쿵쿼퀀퀄퀑퀘퀭퀴퀵퀸퀼큄큅큇큉큐큔큘큠크큭큰클큼큽킁키킥킨킬킴킵킷킹타탁탄탈탉탐탑탓탔탕태택탠탤탬탭탯탰탱탸턍터턱턴털턺텀텁텃텄텅테텍텐텔템텝텟텡텨텬텼톄톈토톡톤톨톰톱톳통톺톼퇀퇘퇴퇸툇툉툐투툭툰툴툼툽툿퉁퉈퉜퉤튀튁튄튈튐튑튕튜튠튤튬튱트특튼튿틀틂틈틉틋틔틘틜틤틥티틱틴틸팀팁팃팅파팍팎판팔팖팜팝팟팠팡팥패팩팬팰팸팹팻팼팽퍄퍅퍼퍽펀펄펌펍펏펐펑페펙펜펠펨펩펫펭펴편펼폄폅폈평폐폘폡폣포폭폰폴폼폽폿퐁퐈퐝푀푄표푠푤푭푯푸푹푼푿풀풂품풉풋풍풔풩퓌퓐퓔퓜퓟퓨퓬퓰퓸퓻퓽프픈플픔픕픗피픽핀필핌핍핏핑하학한할핥함합핫항해핵핸핼햄햅햇했행햐향허헉헌헐헒험헙헛헝헤헥헨헬헴헵헷헹혀혁현혈혐협혓혔형혜혠혤혭호혹혼홀홅홈홉홋홍홑화확환활홧황홰홱홴횃횅회획횐횔횝횟횡효횬횰횹횻후훅훈훌훑훔훗훙훠훤훨훰훵훼훽휀휄휑휘휙휜휠휨휩휫휭휴휵휸휼흄흇흉흐흑흔흖흗흘흙흠흡흣흥흩희흰흴흼흽힁히힉힌힐힘힙힛힝', help='character label')\n",
    "parser.add_argument('--sensitive', action='store_true', help='for sensitive character mode')\n",
    "parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
    "\"\"\" Model Architecture \"\"\"\n",
    "parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
    "parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
    "parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
    "parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
    "parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
    "parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
    "parser.add_argument('--output_channel', type=int, default=512,\n",
    "                    help='the number of output channel of Feature extractor')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "opt = easydict.EasyDict({\n",
    "    \"image_folder\": 'test',\n",
    "    \"workers\": 4,\n",
    "    \"batch_size\": 192,\n",
    "    \"saved_model\": 'best_accuracy.pth',\n",
    "    \"batch_max_length\": 25,\n",
    "    \"imgH\": 32,\n",
    "    \"imgW\": 100,\n",
    "    \"rgb\": False,\n",
    "    \"character\": ' !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmonpqrstuvwxyz{|}~가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰갱갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겄겅겆겉겊겋게겐겔겜겝겟겠겡겨격겪견겯결겸겹겻겼경곁계곈곌곕곗고곡곤곧골곪곬곯곰곱곳공곶과곽관괄괆괌괍괏광괘괜괠괩괬괭괴괵괸괼굄굅굇굉교굔굘굡굣구국군굳굴굵굶굻굼굽굿궁궂궈궉권궐궜궝궤궷귀귁귄귈귐귑귓규균귤그극근귿글긁금급긋긍긔기긱긴긷길긺김깁깃깅깆깊까깍깎깐깔깖깜깝깟깠깡깥깨깩깬깰깸깹깻깼깽꺄꺅꺌꺼꺽꺾껀껄껌껍껏껐껑께껙껜껨껫껭껴껸껼꼇꼈꼍꼐꼬꼭꼰꼲꼴꼼꼽꼿꽁꽂꽃꽈꽉꽐꽜꽝꽤꽥꽹꾀꾄꾈꾐꾑꾕꾜꾸꾹꾼꿀꿇꿈꿉꿋꿍꿎꿔꿜꿨꿩꿰꿱꿴꿸뀀뀁뀄뀌뀐뀔뀜뀝뀨끄끅끈끊끌끎끓끔끕끗끙끝끼끽낀낄낌낍낏낑나낙낚난낟날낡낢남납낫났낭낮낯낱낳내낵낸낼냄냅냇냈냉냐냑냔냘냠냥너넉넋넌널넒넓넘넙넛넜넝넣네넥넨넬넴넵넷넸넹녀녁년녈념녑녔녕녘녜녠노녹논놀놂놈놉놋농높놓놔놘놜놨뇌뇐뇔뇜뇝뇟뇨뇩뇬뇰뇹뇻뇽누눅눈눋눌눔눕눗눙눠눴눼뉘뉜뉠뉨뉩뉴뉵뉼늄늅늉느늑는늘늙늚늠늡늣능늦늪늬늰늴니닉닌닐닒님닙닛닝닢다닥닦단닫달닭닮닯닳담답닷닸당닺닻닿대댁댄댈댐댑댓댔댕댜더덕덖던덛덜덞덟덤덥덧덩덫덮데덱덴델뎀뎁뎃뎄뎅뎌뎐뎔뎠뎡뎨뎬도독돈돋돌돎돐돔돕돗동돛돝돠돤돨돼됐되된될됨됩됫됴두둑둔둘둠둡둣둥둬뒀뒈뒝뒤뒨뒬뒵뒷뒹듀듄듈듐듕드득든듣들듦듬듭듯등듸디딕딘딛딜딤딥딧딨딩딪따딱딴딸땀땁땃땄땅땋때땍땐땔땜땝땟땠땡떠떡떤떨떪떫떰떱떳떴떵떻떼떽뗀뗄뗌뗍뗏뗐뗑뗘뗬또똑똔똘똥똬똴뙈뙤뙨뚜뚝뚠뚤뚫뚬뚱뛔뛰뛴뛸뜀뜁뜅뜨뜩뜬뜯뜰뜸뜹뜻띄띈띌띔띕띠띤띨띰띱띳띵라락란랄람랍랏랐랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝려력련렬렴렵렷렸령례롄롑롓로록론롤롬롭롯롱롸롼뢍뢨뢰뢴뢸룀룁룃룅료룐룔룝룟룡루룩룬룰룸룹룻룽뤄뤘뤠뤼뤽륀륄륌륏륑류륙륜률륨륩륫륭르륵른를름릅릇릉릊릍릎리릭린릴림립릿링마막만많맏말맑맒맘맙맛망맞맡맣매맥맨맬맴맵맷맸맹맺먀먁먈먕머먹먼멀멂멈멉멋멍멎멓메멕멘멜멤멥멧멨멩며멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏뭐뭔뭘뭡뭣뭬뮈뮌뮐뮤뮨뮬뮴뮷므믄믈믐믓미믹민믿밀밂밈밉밋밌밍및밑바박밖밗반받발밝밞밟밤밥밧방밭배백밴밸뱀뱁뱃뱄뱅뱉뱌뱍뱐뱝버벅번벋벌벎범법벗벙벚베벡벤벧벨벰벱벳벴벵벼벽변별볍볏볐병볕볘볜보복볶본볼봄봅봇봉봐봔봤봬뵀뵈뵉뵌뵐뵘뵙뵤뵨부북분붇불붉붊붐붑붓붕붙붚붜붤붰붸뷔뷕뷘뷜뷩뷰뷴뷸븀븃븅브븍븐블븜븝븟비빅빈빌빎빔빕빗빙빚빛빠빡빤빨빪빰빱빳빴빵빻빼빽뺀뺄뺌뺍뺏뺐뺑뺘뺙뺨뻐뻑뻔뻗뻘뻠뻣뻤뻥뻬뼁뼈뼉뼘뼙뼛뼜뼝뽀뽁뽄뽈뽐뽑뽕뾔뾰뿅뿌뿍뿐뿔뿜뿟뿡쀼쁑쁘쁜쁠쁨쁩삐삑삔삘삠삡삣삥사삭삯산삳살삵삶삼삽삿샀상샅새색샌샐샘샙샛샜생샤샥샨샬샴샵샷샹섀섄섈섐섕서석섞섟선섣설섦섧섬섭섯섰성섶세섹센셀셈셉셋셌셍셔셕션셜셤셥셧셨셩셰셴셸솅소속솎손솔솖솜솝솟송솥솨솩솬솰솽쇄쇈쇌쇔쇗쇘쇠쇤쇨쇰쇱쇳쇼쇽숀숄숌숍숏숑수숙순숟술숨숩숫숭숯숱숲숴쉈쉐쉑쉔쉘쉠쉥쉬쉭쉰쉴쉼쉽쉿슁슈슉슐슘슛슝스슥슨슬슭슴습슷승시식신싣실싫심십싯싱싶싸싹싻싼쌀쌈쌉쌌쌍쌓쌔쌕쌘쌜쌤쌥쌨쌩썅써썩썬썰썲썸썹썼썽쎄쎈쎌쏀쏘쏙쏜쏟쏠쏢쏨쏩쏭쏴쏵쏸쐈쐐쐤쐬쐰쐴쐼쐽쑈쑤쑥쑨쑬쑴쑵쑹쒀쒔쒜쒸쒼쓩쓰쓱쓴쓸쓺쓿씀씁씌씐씔씜씨씩씬씰씸씹씻씽아악안앉않알앍앎앓암압앗았앙앝앞애액앤앨앰앱앳앴앵야약얀얄얇얌얍얏양얕얗얘얜얠얩어억언얹얻얼얽얾엄업없엇었엉엊엌엎에엑엔엘엠엡엣엥여역엮연열엶엷염엽엾엿였영옅옆옇예옌옐옘옙옛옜오옥온올옭옮옰옳옴옵옷옹옻와왁완왈왐왑왓왔왕왜왝왠왬왯왱외왹왼욀욈욉욋욍요욕욘욜욤욥욧용우욱운울욹욺움웁웃웅워웍원월웜웝웠웡웨웩웬웰웸웹웽위윅윈윌윔윕윗윙유육윤율윰윱윳융윷으윽은을읊음읍읏응읒읓읔읕읖읗의읜읠읨읫이익인일읽읾잃임입잇있잉잊잎자작잔잖잗잘잚잠잡잣잤장잦재잭잰잴잼잽잿쟀쟁쟈쟉쟌쟎쟐쟘쟝쟤쟨쟬저적전절젊점접젓정젖제젝젠젤젬젭젯젱져젼졀졈졉졌졍졔조족존졸졺좀좁좃종좆좇좋좌좍좔좝좟좡좨좼좽죄죈죌죔죕죗죙죠죡죤죵주죽준줄줅줆줌줍줏중줘줬줴쥐쥑쥔쥘쥠쥡쥣쥬쥰쥴쥼즈즉즌즐즘즙즛증지직진짇질짊짐집짓징짖짙짚짜짝짠짢짤짧짬짭짯짰짱째짹짼쨀쨈쨉쨋쨌쨍쨔쨘쨩쩌쩍쩐쩔쩜쩝쩟쩠쩡쩨쩽쪄쪘쪼쪽쫀쫄쫌쫍쫏쫑쫓쫘쫙쫠쫬쫴쬈쬐쬔쬘쬠쬡쭁쭈쭉쭌쭐쭘쭙쭝쭤쭸쭹쮜쮸쯔쯤쯧쯩찌찍찐찔찜찝찡찢찧차착찬찮찰참찹찻찼창찾채책챈챌챔챕챗챘챙챠챤챦챨챰챵처척천철첨첩첫첬청체첵첸첼쳄쳅쳇쳉쳐쳔쳤쳬쳰촁초촉촌촐촘촙촛총촤촨촬촹최쵠쵤쵬쵭쵯쵱쵸춈추축춘출춤춥춧충춰췄췌췐취췬췰췸췹췻췽츄츈츌츔츙츠측츤츨츰츱츳층치칙친칟칠칡침칩칫칭카칵칸칼캄캅캇캉캐캑캔캘캠캡캣캤캥캬캭컁커컥컨컫컬컴컵컷컸컹케켁켄켈켐켑켓켕켜켠켤켬켭켯켰켱켸코콕콘콜콤콥콧콩콰콱콴콸쾀쾅쾌쾡쾨쾰쿄쿠쿡쿤쿨쿰쿱쿳쿵쿼퀀퀄퀑퀘퀭퀴퀵퀸퀼큄큅큇큉큐큔큘큠크큭큰클큼큽킁키킥킨킬킴킵킷킹타탁탄탈탉탐탑탓탔탕태택탠탤탬탭탯탰탱탸턍터턱턴털턺텀텁텃텄텅테텍텐텔템텝텟텡텨텬텼톄톈토톡톤톨톰톱톳통톺톼퇀퇘퇴퇸툇툉툐투툭툰툴툼툽툿퉁퉈퉜퉤튀튁튄튈튐튑튕튜튠튤튬튱트특튼튿틀틂틈틉틋틔틘틜틤틥티틱틴틸팀팁팃팅파팍팎판팔팖팜팝팟팠팡팥패팩팬팰팸팹팻팼팽퍄퍅퍼퍽펀펄펌펍펏펐펑페펙펜펠펨펩펫펭펴편펼폄폅폈평폐폘폡폣포폭폰폴폼폽폿퐁퐈퐝푀푄표푠푤푭푯푸푹푼푿풀풂품풉풋풍풔풩퓌퓐퓔퓜퓟퓨퓬퓰퓸퓻퓽프픈플픔픕픗피픽핀필핌핍핏핑하학한할핥함합핫항해핵핸핼햄햅햇했행햐향허헉헌헐헒험헙헛헝헤헥헨헬헴헵헷헹혀혁현혈혐협혓혔형혜혠혤혭호혹혼홀홅홈홉홋홍홑화확환활홧황홰홱홴횃횅회획횐횔횝횟횡효횬횰횹횻후훅훈훌훑훔훗훙훠훤훨훰훵훼훽휀휄휑휘휙휜휠휨휩휫휭휴휵휸휼흄흇흉흐흑흔흖흗흘흙흠흡흣흥흩희흰흴흼흽힁히힉힌힐힘힙힛힝',\n",
    "    \"sensitive\": False,\n",
    "    \"PAD\": False,\n",
    "    \"Transformation\": 'TPS',\n",
    "    \"FeatureExtraction\": 'ResNet',\n",
    "    \"SequenceModeling\": 'BiLSTM',\n",
    "    \"Prediction\": 'Attn',\n",
    "    \"num_fiducial\": 20,\n",
    "    \"input_channel\": 1,\n",
    "    \"output_channel\": 512,\n",
    "    \"hidden_size\": 256\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" vocab / character number configuration \"\"\"\n",
    "if opt.sensitive:\n",
    "    opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn\n"
     ]
    }
   ],
   "source": [
    "print(opt.Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input parameters 32 100 20 1 512 256 2447 25 TPS ResNet BiLSTM Attn\n",
      "loading pretrained model from best_accuracy.pth\n",
      "--------------------------------------------------------------------------------\n",
      "image_path               \tpredicted_labels         \tconfidence score\n",
      "--------------------------------------------------------------------------------\n",
      "test\\5484_272549341.jpg  \t화                        \t0.1173\n",
      "test\\abc.jpg             \t이                        \t0.9860\n",
      "test\\demo_1.jpg          \t소문난발                     \t0.4545\n",
      "test\\demo_2.jpg          \t복                        \t0.9994\n",
      "test\\demo_3.jpg          \t황                        \t0.8882\n",
      "test\\demo_4.jpg          \t문                        \t0.9995\n",
      "test\\demo_5.jpg          \t격                        \t0.9644\n",
      "test\\demo_6.jpg          \t황                        \t0.5263\n",
      "test\\demo_7.jpg          \t소문난                      \t0.9875\n",
      "test\\demo_8.jpg          \t즉                        \t0.3193\n",
      "test\\demo_9.jpg          \t문                        \t0.9970\n",
      "test\\demo_10.jpg         \t태                        \t0.9961\n",
      "test\\left.jpg            \t문                        \t0.0659\n",
      "test\\test.jpg            \t표                        \t0.1327\n",
      "test\\test_dak.jpg        \t한은 \"일본군 위원회 부원장장이 국진을 강화한\t0.0000\n",
      "test\\닭칼.jpg              \t닭칼국수                     \t0.5544\n"
     ]
    }
   ],
   "source": [
    "demo(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
